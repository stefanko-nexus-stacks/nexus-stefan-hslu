{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with PySpark\n",
    "\n",
    "This notebook is pre-configured to connect to the **Apache Spark cluster**.\n",
    "The `spark` (SparkSession) and `sc` (SparkContext) variables are automatically available.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify Cluster Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark and sc are auto-injected by the PySpark kernel\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Master:        {sc.master}\")\n",
    "print(f\"App ID:        {sc.applicationId}\")\n",
    "print(f\"App name:      {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"Alice\", \"Engineering\", 85000),\n",
    "    (\"Bob\", \"Marketing\", 72000),\n",
    "    (\"Charlie\", \"Engineering\", 92000),\n",
    "    (\"Diana\", \"Marketing\", 68000),\n",
    "    (\"Eve\", \"Engineering\", 95000),\n",
    "    (\"Frank\", \"Sales\", 78000),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"department\", \"salary\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and sort\n",
    "df.filter(df.salary > 75000).orderBy(\"salary\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation by department\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.groupBy(\"department\").agg(\n",
    "    F.count(\"name\").alias(\"employees\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    ").orderBy(\"department\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as SQL table\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Query with Spark SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT department,\n",
    "           COUNT(*) AS headcount,\n",
    "           ROUND(AVG(salary), 0) AS avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spark SQL Magic Cells\n",
    "\n",
    "Use `%%sparksql` to write SQL directly in a cell (auto-loaded on startup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "SELECT name, department, salary\n",
    "FROM employees\n",
    "WHERE salary > 80000\n",
    "ORDER BY salary DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Hetzner Object Storage (S3)\n\nRead and write data directly to Hetzner Object Storage. The S3 credentials are auto-configured via environment variables. The bucket name is available as `HETZNER_S3_BUCKET`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nbucket = os.environ.get(\"HETZNER_S3_BUCKET\", \"\")\nif bucket:\n    # Write sample data as CSV to S3\n    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(f\"s3a://{bucket}/sample-data/employees\")\n    print(f\"Written to s3a://{bucket}/sample-data/employees/\")\n\n    # Read it back\n    df_s3 = spark.read.csv(f\"s3a://{bucket}/sample-data/employees\", header=True, inferSchema=True)\n    df_s3.show()\nelse:\n    print(\"Hetzner Object Storage not configured (HETZNER_S3_BUCKET is empty)\")"
  },
  {
   "cell_type": "markdown",
   "id": "uft1gcjiy4",
   "source": "## 7. Cluster Info\n\nCheck the **Spark Master Web UI** for detailed cluster monitoring:\n- Workers, running applications, completed jobs\n- Available at `https://spark.YOUR_DOMAIN`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gyaymxp60yn",
   "source": "# Cluster resources\nprint(f\"Default parallelism: {sc.defaultParallelism}\")\nprint(f\"Spark UI: {sc.uiWebUrl}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Spark Cluster)",
   "language": "python",
   "name": "pyspark_cluster"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}